# --- START OF FILE tracker.py ---
# START: MODIFIED SECTION
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import subprocess
from datetime import datetime
from pathlib import Path
import re
from typing import List, Dict, Tuple
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from queue import Queue

class TikTokTracker:
    def __init__(self):
        self.users_file = "users.txt"
        self.data_file = "tiktok_data.json"
        self.thumbnails_dir = "thumbnails"
        self.html_file = "index.html"
        self.temp_data_file = "tiktok_data_temp.json"  # Ù…Ù„Ù Ù…Ø¤Ù‚Øª Ù„Ù„Ø­ÙØ¸ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµÙˆØ± Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        Path(self.thumbnails_dir).mkdir(exist_ok=True)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¥Ù† ÙˆØ¬Ø¯Øª
        self.data = self.load_data()
        
        # Lock Ù„Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¢Ù…Ù† Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¹Ø¯Ø© threads
        self.data_lock = threading.Lock()
        self.save_lock = threading.Lock()
        self.print_lock = threading.Lock()
        
        # Ø¹Ø¯Ø§Ø¯ Ù„Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.processed_videos_count = 0
        self.total_new_videos = 0
    
    def safe_print(self, message: str):
        """Ø·Ø¨Ø§Ø¹Ø© Ø¢Ù…Ù†Ø© Ù…Ù† Ø¹Ø¯Ø© threads"""
        with self.print_lock:
            print(message)
    
    def load_data(self) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª Ø£ÙˆÙ„Ø§Ù‹ (ÙÙŠ Ø­Ø§Ù„Ø© ØªÙˆÙ‚Ù Ù…ÙØ§Ø¬Ø¦)
        if os.path.exists(self.temp_data_file):
            try:
                with open(self.temp_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.safe_print(f"âš ï¸ Found temporary data file. Recovering from previous session...")
                # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª Ù„ÙŠØµØ¨Ø­ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                os.rename(self.temp_data_file, self.data_file)
                return data
            except:
                pass
        
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                self.safe_print(f"âš ï¸ Warning: Data file {self.data_file} is corrupted. Starting fresh.")
                return {"users": {}}
            except Exception as e:
                self.safe_print(f"âŒ Unexpected error while reading {self.data_file}: {e}")
                return {"users": {}}
        return {"users": {}}
    
    def save_data(self, is_temp=False):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù JSON"""
        with self.save_lock:
            filename = self.temp_data_file if is_temp else self.data_file
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(self.data, f, ensure_ascii=False, indent=2)
                
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØŒ Ø§Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
                if not is_temp and os.path.exists(self.temp_data_file):
                    os.remove(self.temp_data_file)
            except Exception as e:
                self.safe_print(f"âŒ Error saving data to {filename}: {e}")
    
    def get_users(self) -> List[str]:
        """Ù‚Ø±Ø§Ø¡Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        if not os.path.exists(self.users_file):
            self.safe_print(f"âŒ File not found: {self.users_file}")
            return []
        
        with open(self.users_file, 'r', encoding='utf-8') as f:
            users = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        return users
    
    def extract_video_id(self, url: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        match = re.search(r'/video/(\d+)', url)
        if match:
            return match.group(1)
        return url.split('/')[-1]
    
    def download_thumbnail_single(self, video_url: str, username: str, video_num: int = 0) -> str:
        """ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù…ØµØºØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ù„ÙÙŠØ¯ÙŠÙˆ"""
        video_id = self.extract_video_id(video_url)
        base_filename = f"{username}_{video_id}"
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„ÙØ¹Ù„
        for ext in ['.jpg', '.jpeg', '.webp', '.png', '.image']:
            existing_path = os.path.join(self.thumbnails_dir, base_filename + ext)
            if os.path.exists(existing_path):
                # NOTE: ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø·Ø¨Ø§Ø¹Ø© Ù„ØªÙƒÙˆÙ† Ø£Ù‚Ù„ Ø¥Ø²Ø¹Ø§Ø¬Ù‹Ø§
                # self.safe_print(f"      âœ“ Thumbnail already exists for video {video_num}: {video_id}")
                return existing_path
        
        # NOTE: ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø·Ø¨Ø§Ø¹Ø© Ù„ØªÙƒÙˆÙ† Ø£Ù‚Ù„ Ø¥Ø²Ø¹Ø§Ø¬Ù‹Ø§
        # self.safe_print(f"      ğŸ“· Downloading thumbnail for video {video_num}: {video_id}")
        start_time = time.time()
        
        try:
            output_template = os.path.join(self.thumbnails_dir, base_filename)
            cmd = [
                "yt-dlp",
                "--write-thumbnail", "--skip-download", "--convert-thumbnails", "jpg",
                "-o", output_template,
                "--quiet",  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
                video_url
            ]
            subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', check=True, timeout=30)
            
            elapsed = time.time() - start_time
            
            final_path = output_template + '.jpg'
            if os.path.exists(final_path):
                # self.safe_print(f"      âœ… Thumbnail downloaded for video {video_num}: {video_id} ({elapsed:.2f}s)")
                return final_path

            for ext in ['.jpeg', '.webp', '.png', '.image']:
                fallback_path = output_template + ext
                if os.path.exists(fallback_path):
                    # self.safe_print(f"      âœ… Thumbnail downloaded for video {video_num}: {video_id} ({elapsed:.2f}s)")
                    return fallback_path
            
            self.safe_print(f"      âš ï¸ Could not find thumbnail for video {video_num}: {video_id}")
            return None

        except subprocess.TimeoutExpired:
            self.safe_print(f"      âš ï¸ Timeout downloading thumbnail for video {video_num}: {video_id}")
            return None
        except Exception as e:
            self.safe_print(f"      âš ï¸ Failed to download thumbnail for video {video_num}: {video_id} - {str(e)[:50]}")
            return None
    
    # --- START: NEW/MODIFIED FUNCTIONS FOR SPEED ---

    def _producer_fetch_links(self, username: str, existing_ids: set, link_queue: Queue):
        """
        [Producer] ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù…ÙÙ†ØªÙØ¬: ØªØ¬Ù„Ø¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø³Ø±Ø¹Ø© ÙˆØªØ¶Ø¹Ù‡Ø§ ÙÙŠ Ø§Ù„Ø·Ø§Ø¨ÙˆØ±.
        """
        self.safe_print(f"ğŸ“¥ Starting to fetch video links for @{username}...")
        fetch_start_time = time.time()
        video_counter = 0
        duplicate_count = 0
        
        url = f"https://www.tiktok.com/@{username}"
        cmd = ["yt-dlp", "--simulate", "--print", "%(webpage_url)s", url]
        
        process = None
        try:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, 
                                     text=True, encoding='utf-8')

            for line in process.stdout:
                link = line.strip()
                if not link:
                    continue

                video_counter += 1
                video_id = self.extract_video_id(link)
                
                if video_id in existing_ids:
                    duplicate_count += 1
                    # self.safe_print(f"    - Checking video {video_counter}: ID {video_id} (DUPLICATE #{duplicate_count})")
                    if duplicate_count >= 10: # ØªÙ… Ø²ÙŠØ§Ø¯Ø© Ø­Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©
                        self.safe_print(f"    âœ… Found 10 consecutive duplicates. Stopping link fetch for @{username}")
                        break
                else:
                    duplicate_count = 0
                    # self.safe_print(f"    - Checking video {video_counter}: ID {video_id} (NEW)")
                    link_queue.put((link, video_id, video_counter))

            fetch_elapsed = time.time() - fetch_start_time
            self.safe_print(f"ğŸ“Š Link fetch completed in {fetch_elapsed:.2f} seconds. Found {link_queue.qsize()} new videos to process.")
        except Exception as e:
            self.safe_print(f"âŒ Error during link fetching for @{username}: {e}")
        finally:
            if process:
                process.terminate()
                process.wait(timeout=5)
            # ÙˆØ¶Ø¹ Ø¹Ù„Ø§Ù…Ø© Ø®Ø§ØµØ© ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø·Ø§Ø¨ÙˆØ± Ù„Ø¥Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‡Ù„ÙƒÙŠÙ† Ø¨Ø§Ù„ØªÙˆÙ‚Ù
            link_queue.put(None)

    def _consumer_process_videos(self, username: str, link_queue: Queue, user_data: dict):
        """
        [Consumer] ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù…Ø³ØªÙ‡ÙÙ„ÙÙƒ: ØªØ³Ø­Ø¨ Ø±Ø§Ø¨Ø·Ù‹Ø§ Ù…Ù† Ø§Ù„Ø·Ø§Ø¨ÙˆØ±ØŒ ØªØ­Ù…Ù„ ØµÙˆØ±ØªÙ‡ØŒ ÙˆØªØ­Ø¯Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
        """
        processed_count = 0
        while True:
            item = link_queue.get()
            if item is None: # Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
                link_queue.put(None) # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¹Ù„Ø§Ù…Ø© Ù„Ù„Ø¹Ù…Ø§Ù„ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†
                break

            link, video_id, video_num = item
            
            thumbnail_path = self.download_thumbnail_single(link, username, video_num)
            
            with self.data_lock:
                user_data["videos"][video_id] = {
                    "url": link,
                    "thumbnail": thumbnail_path,
                    "added_date": datetime.now().isoformat(),
                    "is_new": True
                }
            
            processed_count += 1
            # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†
            with self.data_lock:
                 self.total_new_videos += 1

            # Ø­ÙØ¸ ØªØ¯Ø±ÙŠØ¬ÙŠ
            if processed_count % 50 == 0:
                self.safe_print(f"    ğŸ’¾ Auto-saving progress ({processed_count} new videos processed for @{username})...")
                self.save_data(is_temp=True)
            
            link_queue.task_done()

    def process_user(self, username: str, user_number: int, total_users: int):
        """
        [MODIFIED] ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„ØªØ³ØªØ®Ø¯Ù… Ù…Ù†Ø·Ù‚ Ø§Ù„Ù…ÙÙ†ØªÙØ¬/Ø§Ù„Ù…Ø³ØªÙ‡Ù„ÙÙƒ.
        """
        self.safe_print(f"\n{'='*60}")
        self.safe_print(f"ğŸ”„ Processing user [{user_number}/{total_users}]: @{username}")
        self.safe_print(f"{'='*60}")
        
        process_start_time = time.time()

        with self.data_lock:
            if username not in self.data["users"]:
                self.data["users"][username] = {"videos": {}, "last_update": None}
            user_data = self.data["users"][username]
            existing_ids = set(user_data.get("videos", {}).keys())
        
        self.safe_print(f"ğŸ“Š Current videos in database: {len(existing_ids)}")
        
        link_queue = Queue()
        
        # --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙÙ†ØªÙØ¬ ÙˆØ§Ù„Ù…Ø³ØªÙ‡Ù„ÙƒÙŠÙ† ÙÙŠ threads Ù…Ù†ÙØµÙ„Ø© ---
        producer_thread = threading.Thread(target=self._producer_fetch_links, args=(username, existing_ids, link_queue))
        producer_thread.start()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø¹Ù…Ø§Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± (Ø§Ù„Ù…Ø³ØªÙ‡Ù„ÙƒÙŠÙ†)
        # ğŸš€ğŸš€ğŸš€ ØªÙ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ Ø¥Ù„Ù‰ 64 ÙƒÙ…Ø§ Ø·Ù„Ø¨Øª ğŸš€ğŸš€ğŸš€
        num_consumers = 64
        self.safe_print(f"    ğŸš€ Starting {num_consumers} parallel thumbnail downloaders...")
        
        consumer_threads = []
        for _ in range(num_consumers):
            thread = threading.Thread(target=self._consumer_process_videos, args=(username, link_queue, user_data))
            thread.start()
            consumer_threads.append(thread)
            
        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠÙ†ØªÙ‡ÙŠ Ø§Ù„Ù…ÙÙ†ØªÙØ¬ Ù…Ù† Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        producer_thread.join()
        
        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠÙØ±Øº Ø§Ù„Ø·Ø§Ø¨ÙˆØ± (Ø­ØªÙ‰ ÙŠØ¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³ØªÙ‡Ù„ÙƒÙˆÙ† ÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·)
        link_queue.join()
        
        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ØªÙ†ØªÙ‡ÙŠ Ø¬Ù…ÙŠØ¹ threads Ø§Ù„Ù…Ø³ØªÙ‡Ù„ÙƒØ©
        for thread in consumer_threads:
            thread.join()

        self.safe_print(f"    âœ… All new videos for @{username} have been processed.")

        # --- ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---
        with self.data_lock:
            # ØªØ­Ø¯ÙŠØ« Ø¹Ù„Ø§Ù…Ø© "is_new" Ù„Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            for video_data in user_data["videos"].values():
                if "is_new" in video_data: # ØªØ­Ù‚Ù‚ Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¬Ø¯Ø§Ù‹
                     video_data["is_new"] = False
            
            user_data["last_update"] = datetime.now().isoformat()
            self.save_data(is_temp=True)
        
        total_elapsed = time.time() - process_start_time
        self.safe_print(f"âœ… Completed @{username} in {total_elapsed:.2f} seconds")
        self.safe_print(f"ğŸ“Š Total videos for @{username}: {len(user_data['videos'])}")

    # --- END: NEW/MODIFIED FUNCTIONS FOR SPEED ---
    
    def process_users_parallel(self, users: List[str]):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ (3 ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª)"""
        total_users = len(users)
        self.safe_print(f"\nğŸš€ Starting parallel processing of {total_users} users (3 concurrent)...")
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {
                executor.submit(self.process_user, username, idx+1, total_users): username 
                for idx, username in enumerate(users)
            }
            
            completed_count = 0
            for future in as_completed(futures):
                username = futures[future]
                completed_count += 1
                try:
                    future.result()
                    self.safe_print(f"\nâœ… [{completed_count}/{total_users}] Completed processing @{username}")
                except Exception as e:
                    self.safe_print(f"\nâŒ [{completed_count}/{total_users}] Error processing @{username}: {e}")
# END: MODIFIED SECTION